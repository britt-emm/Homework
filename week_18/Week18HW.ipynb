{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfb26bc",
   "metadata": {},
   "source": [
    "## 1. What is a neural network? What are the general steps required to build a neural network?\n",
    "\n",
    "A neural network is a type of machine learning that is designed to mimic the brain's neural network. It can utilize hidden layers with hundreds of nodes and also use backpropagation to improve the network.\n",
    "\n",
    "- Specify architecture: how many hidden layers, input shape, how many nodes, what sort of activation\n",
    "- Compile: specifiy optimizer, learning rate(if desired), loss function\n",
    "- Fit: this is where the backpropagation comes in to adjust the weights of the nodes. You can also define the split of the data here instead of splitting it in advance, and specify the number of epochs to use when fitting.\n",
    "- Predict: actually make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918ed35",
   "metadata": {},
   "source": [
    "## 2. Generally, how do you check the performance of a neural network? Why?\n",
    "\n",
    "Using the loss function, it calculates how the network is performing during each instance/epoch and you can use it to determine when to stop running epochs because the model is doing as well as it can. You also need to do a validation split to make sure that you can check how it performs on unseen data. Also training on the full dataset is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcabc2a",
   "metadata": {},
   "source": [
    "## 3. Create a neural network using keras to predict the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55536be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e080ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"../abalone.data\") as infile, open(\"abalone.csv\", \"w\") as outfile:\n",
    "    csv_writer = csv.writer(outfile, delimiter=',')\n",
    "    #create a row of titles\n",
    "    csv_writer.writerow(['Sex','Length','Diameter','Height','Whole weight','Shucked weight',\n",
    "                         'Viscera weight','Shell weight','Rings'])\n",
    "    for line in infile:\n",
    "        #check to see how the data is divided so you know what to split the line on\n",
    "        row = [field.strip() for field in line.split(',')]\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b423143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_df = pd.read_csv('abalone.csv')\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a6e699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
       "0    2   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1    2   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2    0   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3    2   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4    1   0.330     0.255   0.080        0.2050          0.0895   \n",
       "\n",
       "   Viscera weight  Shell weight  Rings  \n",
       "0          0.1010         0.150     15  \n",
       "1          0.0485         0.070      7  \n",
       "2          0.1415         0.210      9  \n",
       "3          0.1140         0.155     10  \n",
       "4          0.0395         0.055      7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeated preprocessing from last week\n",
    "#nominal values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sex_labels = LabelEncoder()\n",
    "abalone_df['Sex'] = sex_labels.fit_transform(abalone_df['Sex'].values)\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46f25bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
       "0    2   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1    2   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2    0   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3    2   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4    1   0.330     0.255   0.080        0.2050          0.0895   \n",
       "\n",
       "   Viscera weight  Shell weight  Rings   age  \n",
       "0          0.1010         0.150     15  16.5  \n",
       "1          0.0485         0.070      7   8.5  \n",
       "2          0.1415         0.210      9  10.5  \n",
       "3          0.1140         0.155     10  11.5  \n",
       "4          0.0395         0.055      7   8.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_df['age']=abalone_df.apply(lambda x: x['Rings']+1.5,axis=1)\n",
    "abalone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0dd52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 8) (4177,)\n"
     ]
    }
   ],
   "source": [
    "# define data \n",
    "to_drop = abalone_df[['Rings', 'age']]\n",
    "predictors = np.array(abalone_df.drop(to_drop, axis =1))\n",
    "target = np.array(abalone_df['age'])\n",
    "print(predictors.shape, target.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "764638cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 1s 4ms/step - loss: 125.9459 - val_loss: 47.1845\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 33.1986 - val_loss: 10.2421\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 11.3501 - val_loss: 9.2312\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 10.0357 - val_loss: 8.1480\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 9.5565 - val_loss: 7.4883\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 9.4283 - val_loss: 6.8818\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 8.7298 - val_loss: 6.5420\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 7.8549 - val_loss: 6.3280\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 7.1701 - val_loss: 5.9431\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 7.2309 - val_loss: 5.7603\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.5393 - val_loss: 5.5832\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 6.8423 - val_loss: 5.6634\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 6.8179 - val_loss: 5.4240\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 7.0476 - val_loss: 5.2778\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.5857 - val_loss: 5.2173\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.4832 - val_loss: 5.1457\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.5456 - val_loss: 5.0120\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.3812 - val_loss: 4.8848\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.3369 - val_loss: 4.7898\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.6000 - val_loss: 4.7449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc56a7c2340>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "\n",
    "n_cols= predictors.shape[1]\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "#instantiate model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape= (n_cols,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=20, \n",
    "          callbacks= [early_stopping_monitor])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ecc0423",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 1s 4ms/step - loss: 96.9519 - val_loss: 9.4106\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 10.4216 - val_loss: 6.8387\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 7.9272 - val_loss: 5.8116\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 7.2933 - val_loss: 5.4144\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 6.4259 - val_loss: 5.0050\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 6.5890 - val_loss: 4.9748\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.1966 - val_loss: 5.1443\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 6.2496 - val_loss: 4.5407\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 5.5317 - val_loss: 4.5162\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 5.6967 - val_loss: 4.4776\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.1246 - val_loss: 4.3555\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.1082 - val_loss: 4.3035\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.4285 - val_loss: 4.2242\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.9593 - val_loss: 4.6337\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.5205 - val_loss: 4.2202\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.1745 - val_loss: 4.4887\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 5.2938 - val_loss: 4.2527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc56b9661c0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a second model to compare\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(100, activation='relu', input_shape= (n_cols,)))\n",
    "model_1.add(Dense(100, activation='relu'))\n",
    "model_1.add(Dense(1))\n",
    "\n",
    "model_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_1.fit(predictors, target, validation_split=0.3, epochs=20, \n",
    "          callbacks= [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc26cfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 79.3856 - val_loss: 7.6716\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 8.6286 - val_loss: 5.5948\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.9933 - val_loss: 5.3005\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.5662 - val_loss: 5.1210\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.9898 - val_loss: 4.6682\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.1278 - val_loss: 4.5940\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.8036 - val_loss: 4.7147\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.7999 - val_loss: 4.4727\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.3727 - val_loss: 4.4127\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.7419 - val_loss: 4.3304\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.0073 - val_loss: 4.1484\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 5.0704 - val_loss: 4.1434\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 4.8527 - val_loss: 4.2140\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.9804 - val_loss: 4.1050\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.8536 - val_loss: 4.0711\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.1673 - val_loss: 4.1045\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 5.3205 - val_loss: 4.5848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc56674e1f0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a third model to compare\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(150, activation='relu', input_shape= (n_cols,)))\n",
    "model_2.add(Dense(150, activation='relu'))\n",
    "model_2.add(Dense(1))\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_2.fit(predictors, target, validation_split=0.3, epochs=20, \n",
    "          callbacks= [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710e107",
   "metadata": {},
   "source": [
    "### Best performing keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d93c0c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 70.7295 - val_loss: 6.6344\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 7.7968 - val_loss: 5.1957\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.7987 - val_loss: 7.0635\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.2820 - val_loss: 4.5976\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 6.0704 - val_loss: 4.5520\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.3730 - val_loss: 4.5614\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.2370 - val_loss: 4.5059\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 5.4572 - val_loss: 4.5144\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.1289 - val_loss: 4.4518\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.4179 - val_loss: 4.4276\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.9973 - val_loss: 4.2978\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 5.0824 - val_loss: 4.0547\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 4.6667 - val_loss: 4.0226\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.1425 - val_loss: 3.9991\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.8338 - val_loss: 4.1458\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.8466 - val_loss: 3.9950\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 5.0197 - val_loss: 4.2285\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 4.6928 - val_loss: 3.9634\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.8033 - val_loss: 4.3310\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 0s 3ms/step - loss: 4.9600 - val_loss: 3.9522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc568d95220>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fourth model \n",
    "# create model\n",
    "\n",
    "n_cols= predictors.shape[1]\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(150, activation='relu', input_shape= (n_cols,)))\n",
    "model_3.add(Dense(150, activation='relu'))\n",
    "model_3.add(Dense(150, activation='relu'))\n",
    "model_3.add(Dense(1))\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_3.fit(predictors, target, validation_split=0.3, epochs=20, \n",
    "          callbacks= [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f50fa16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 150)               1350      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 46,801\n",
      "Trainable params: 46,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b79e1",
   "metadata": {},
   "source": [
    "## 4. Write another algorithm using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9a6e904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 8) (4177,)\n",
      "(2923, 8) (2923,)\n",
      "(1254, 8) (1254,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = np.array(abalone_df.drop(to_drop, axis =1))\n",
    "y = np.array(abalone_df['age']).reshape(-1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "#Standardize  The model performed better without standardization\n",
    "#sc= StandardScaler()\n",
    "#X_train=sc.fit_transform(X_train)\n",
    "#X_test=sc.fit_transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7fc3e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5469333028936754"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate knn\n",
    "\n",
    "knn= KNeighborsRegressor(n_neighbors=20)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09bf0700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.600691786283892"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred= knn.predict(X_test)\n",
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d2b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4177, 8) (4177,)\n",
      "(2923, 8) (2923,)\n",
      "(1254, 8) (1254,)\n"
     ]
    }
   ],
   "source": [
    "# try another KNN model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "to_drop = abalone_df[['Rings', 'age']]\n",
    "X = np.array(abalone_df.drop(to_drop, axis =1))\n",
    "y = np.array(abalone_df['age']).reshape(-1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "#Standardize  The model performed better without standardization\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "348eebb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.811446925394294"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate knn\n",
    "\n",
    "knn= KNeighborsRegressor(n_neighbors=30, algorithm='kd_tree', leaf_size=50)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred= knn.predict(X_test)\n",
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af720206",
   "metadata": {},
   "source": [
    "## 5. Create a neural network using pytorch to predict the same result as question 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18db637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2923, 8) (1254, 8) (2923,) (1254,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = abalone_df.drop(to_drop, axis=1).values\n",
    "y = abalone_df['age'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "#Standardize\n",
    "#sc= StandardScaler()\n",
    "#X_train=sc.fit_transform(X_train)\n",
    "#X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f742375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.5250, 0.4300,  ..., 0.4325, 0.1800, 0.1815],\n",
      "        [1.0000, 0.4300, 0.3250,  ..., 0.1575, 0.0825, 0.1050],\n",
      "        [2.0000, 0.4550, 0.3500,  ..., 0.1625, 0.0970, 0.1450],\n",
      "        ...,\n",
      "        [2.0000, 0.5100, 0.3950,  ..., 0.2440, 0.1335, 0.1880],\n",
      "        [2.0000, 0.5750, 0.4650,  ..., 0.5160, 0.2185, 0.2350],\n",
      "        [0.0000, 0.5950, 0.4750,  ..., 0.5470, 0.2310, 0.2710]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #this has activation functions\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Creating tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "#I think these need to be float tensors rather than long tensors because it's regression\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73f4e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=100, out_features =1):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        #self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden1, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        #x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75543688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN3_Model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, hidden3=20, out_features =1):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.layer_3_connection = nn.Linear(hidden2, hidden3)\n",
    "        self.out = nn.Linear(hidden3, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = F.relu(self.layer_3_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "28f6b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0429acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6a121526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 10.486957550048828\n",
      "Epoch number: 11 with loss: 10.486957550048828\n",
      "Epoch number: 21 with loss: 10.486957550048828\n",
      "Epoch number: 31 with loss: 10.486957550048828\n",
      "Epoch number: 41 with loss: 10.486957550048828\n",
      "Epoch number: 51 with loss: 10.486957550048828\n",
      "Epoch number: 61 with loss: 10.486956596374512\n",
      "Epoch number: 71 with loss: 10.486956596374512\n",
      "Epoch number: 81 with loss: 10.486955642700195\n",
      "Epoch number: 91 with loss: 10.486955642700195\n",
      "Epoch number: 101 with loss: 10.486955642700195\n",
      "Epoch number: 111 with loss: 10.486955642700195\n",
      "Epoch number: 121 with loss: 10.486953735351562\n",
      "Epoch number: 131 with loss: 10.486953735351562\n",
      "Epoch number: 141 with loss: 10.486953735351562\n",
      "Epoch number: 151 with loss: 10.486952781677246\n",
      "Epoch number: 161 with loss: 10.486952781677246\n",
      "Epoch number: 171 with loss: 10.486952781677246\n",
      "Epoch number: 181 with loss: 10.486952781677246\n",
      "Epoch number: 191 with loss: 10.486952781677246\n",
      "Epoch number: 201 with loss: 10.486952781677246\n",
      "Epoch number: 211 with loss: 10.486952781677246\n",
      "Epoch number: 221 with loss: 10.48695182800293\n",
      "Epoch number: 231 with loss: 10.48695182800293\n",
      "Epoch number: 241 with loss: 10.48695182800293\n",
      "Epoch number: 251 with loss: 10.48695182800293\n",
      "Epoch number: 261 with loss: 10.48695182800293\n",
      "Epoch number: 271 with loss: 10.486950874328613\n",
      "Epoch number: 281 with loss: 10.486950874328613\n",
      "Epoch number: 291 with loss: 10.486950874328613\n",
      "Epoch number: 301 with loss: 10.486950874328613\n",
      "Epoch number: 311 with loss: 10.486949920654297\n",
      "Epoch number: 321 with loss: 10.48694896697998\n",
      "Epoch number: 331 with loss: 10.48694896697998\n",
      "Epoch number: 341 with loss: 10.48694896697998\n",
      "Epoch number: 351 with loss: 10.48694896697998\n",
      "Epoch number: 361 with loss: 10.48694896697998\n",
      "Epoch number: 371 with loss: 10.48694896697998\n",
      "Epoch number: 381 with loss: 10.486947059631348\n",
      "Epoch number: 391 with loss: 10.486947059631348\n",
      "Epoch number: 401 with loss: 10.486947059631348\n",
      "Epoch number: 411 with loss: 10.486947059631348\n",
      "Epoch number: 421 with loss: 10.486947059631348\n",
      "Epoch number: 431 with loss: 10.486947059631348\n",
      "Epoch number: 441 with loss: 10.486947059631348\n",
      "Epoch number: 451 with loss: 10.486945152282715\n",
      "Epoch number: 461 with loss: 10.486945152282715\n",
      "Epoch number: 471 with loss: 10.486945152282715\n",
      "Epoch number: 481 with loss: 10.486945152282715\n",
      "Epoch number: 491 with loss: 10.486945152282715\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c2f9ec",
   "metadata": {},
   "source": [
    "### Best pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6defbfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 57614.45703125\n",
      "Epoch number: 11 with loss: 10.486570358276367\n",
      "Epoch number: 21 with loss: 10.486570358276367\n",
      "Epoch number: 31 with loss: 10.486570358276367\n",
      "Epoch number: 41 with loss: 10.486570358276367\n",
      "Epoch number: 51 with loss: 10.486570358276367\n",
      "Epoch number: 61 with loss: 10.486570358276367\n",
      "Epoch number: 71 with loss: 10.486570358276367\n",
      "Epoch number: 81 with loss: 10.486570358276367\n",
      "Epoch number: 91 with loss: 10.486570358276367\n",
      "Epoch number: 101 with loss: 10.486570358276367\n",
      "Epoch number: 111 with loss: 10.486570358276367\n",
      "Epoch number: 121 with loss: 10.486570358276367\n",
      "Epoch number: 131 with loss: 10.486570358276367\n",
      "Epoch number: 141 with loss: 10.486570358276367\n",
      "Epoch number: 151 with loss: 10.486570358276367\n",
      "Epoch number: 161 with loss: 10.486570358276367\n",
      "Epoch number: 171 with loss: 10.486570358276367\n",
      "Epoch number: 181 with loss: 10.486570358276367\n",
      "Epoch number: 191 with loss: 10.486570358276367\n",
      "Epoch number: 201 with loss: 10.486570358276367\n",
      "Epoch number: 211 with loss: 10.486570358276367\n",
      "Epoch number: 221 with loss: 10.486570358276367\n",
      "Epoch number: 231 with loss: 10.486570358276367\n",
      "Epoch number: 241 with loss: 10.486570358276367\n",
      "Epoch number: 251 with loss: 10.486570358276367\n",
      "Epoch number: 261 with loss: 10.486570358276367\n",
      "Epoch number: 271 with loss: 10.486570358276367\n",
      "Epoch number: 281 with loss: 10.486570358276367\n",
      "Epoch number: 291 with loss: 10.486570358276367\n",
      "Epoch number: 301 with loss: 10.486570358276367\n",
      "Epoch number: 311 with loss: 10.486570358276367\n",
      "Epoch number: 321 with loss: 10.486570358276367\n",
      "Epoch number: 331 with loss: 10.486570358276367\n",
      "Epoch number: 341 with loss: 10.486570358276367\n",
      "Epoch number: 351 with loss: 10.486570358276367\n",
      "Epoch number: 361 with loss: 10.486570358276367\n",
      "Epoch number: 371 with loss: 10.486570358276367\n",
      "Epoch number: 381 with loss: 10.486570358276367\n",
      "Epoch number: 391 with loss: 10.486570358276367\n",
      "Epoch number: 401 with loss: 10.486570358276367\n",
      "Epoch number: 411 with loss: 10.486570358276367\n",
      "Epoch number: 421 with loss: 10.486570358276367\n",
      "Epoch number: 431 with loss: 10.486570358276367\n",
      "Epoch number: 441 with loss: 10.486570358276367\n",
      "Epoch number: 451 with loss: 10.486570358276367\n",
      "Epoch number: 461 with loss: 10.486570358276367\n",
      "Epoch number: 471 with loss: 10.486570358276367\n",
      "Epoch number: 481 with loss: 10.486570358276367\n",
      "Epoch number: 491 with loss: 10.486570358276367\n"
     ]
    }
   ],
   "source": [
    "# try a different optimizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.5)\n",
    "\n",
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54544bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 139.0191192626953\n",
      "Epoch number: 11 with loss: 14.874842643737793\n",
      "Epoch number: 21 with loss: 14.058676719665527\n",
      "Epoch number: 31 with loss: 11.880428314208984\n",
      "Epoch number: 41 with loss: 10.91496753692627\n",
      "Epoch number: 51 with loss: 10.595457077026367\n",
      "Epoch number: 61 with loss: 10.515827178955078\n",
      "Epoch number: 71 with loss: 10.49810791015625\n",
      "Epoch number: 81 with loss: 10.493562698364258\n",
      "Epoch number: 91 with loss: 10.49200439453125\n",
      "Epoch number: 101 with loss: 10.491305351257324\n",
      "Epoch number: 111 with loss: 10.49091911315918\n",
      "Epoch number: 121 with loss: 10.49066162109375\n",
      "Epoch number: 131 with loss: 10.490461349487305\n",
      "Epoch number: 141 with loss: 10.490291595458984\n",
      "Epoch number: 151 with loss: 10.490139961242676\n",
      "Epoch number: 161 with loss: 10.489996910095215\n",
      "Epoch number: 171 with loss: 10.489867210388184\n",
      "Epoch number: 181 with loss: 10.489747047424316\n",
      "Epoch number: 191 with loss: 10.489635467529297\n"
     ]
    }
   ],
   "source": [
    "# try a different model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN3_Model()\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a1b3e",
   "metadata": {},
   "source": [
    "## Full disclosure: Completed this section after the assignment was due\n",
    "#### After looking at the reading on normalizing and standardizing the data I wanted to see what would happen if I tried using the minmaxscaler on the input and output data and WOW did it make a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1e79346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2923, 8) (1254, 8) (2923,) (1254,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = abalone_df.drop(to_drop, axis=1).values\n",
    "y = abalone_df['age'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "#histogram of one of the input variables\n",
    "plt.subplot(121)\n",
    "plt.hist(X[:, 0])\n",
    "plt.subplot(122)\n",
    "plt.hist(X[:, 1])\n",
    "plt.show()\n",
    "# histogram of target variable\n",
    "plt.hist(y)\n",
    "plt.show()\n",
    "\n",
    "'''Because all of the features in the training set do NOT have Gaussian distributions \n",
    "using a standard scaler will not be helpful'''\n",
    "#Normalize input data\n",
    "mms = MinMaxScaler()\n",
    "X_train=mms.fit_transform(X_train)\n",
    "X_test=mms.fit_transform(X_test)\n",
    "\n",
    "\n",
    "#Normalized rather than standardize since the data is slightly skewed although standardized might also work I think\n",
    "#Normalize output data\n",
    "# reshape 1d arrays to 2d arrays\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "#fit train data\n",
    "mms.fit(y_train)\n",
    "# transform training dataset\n",
    "y_train = mms.transform(y_train)\n",
    "# transform test dataset\n",
    "y_test = mms.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "004eaba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.6081, 0.6303,  ..., 0.2902, 0.2363, 0.1794],\n",
      "        [0.5000, 0.4797, 0.4538,  ..., 0.1052, 0.1080, 0.1031],\n",
      "        [1.0000, 0.5135, 0.4958,  ..., 0.1086, 0.1271, 0.1430],\n",
      "        ...,\n",
      "        [1.0000, 0.5878, 0.5714,  ..., 0.1634, 0.1751, 0.1858],\n",
      "        [1.0000, 0.6757, 0.6891,  ..., 0.3463, 0.2870, 0.2327],\n",
      "        [0.0000, 0.7027, 0.7059,  ..., 0.3672, 0.3035, 0.2686]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #this has activation functions\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Creating tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "#I think these need to be float tensors rather than long tensors because it's regression\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "567ffeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=100, out_features =1):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        #self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden1, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        #x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46af7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 0.09181395918130875\n",
      "Epoch number: 11 with loss: 0.01913786679506302\n",
      "Epoch number: 21 with loss: 0.016999928280711174\n",
      "Epoch number: 31 with loss: 0.012574262917041779\n",
      "Epoch number: 41 with loss: 0.010284473188221455\n",
      "Epoch number: 51 with loss: 0.009650393389165401\n",
      "Epoch number: 61 with loss: 0.009361721575260162\n",
      "Epoch number: 71 with loss: 0.009158608503639698\n",
      "Epoch number: 81 with loss: 0.008995882235467434\n",
      "Epoch number: 91 with loss: 0.008770848624408245\n",
      "Epoch number: 101 with loss: 0.0085642971098423\n",
      "Epoch number: 111 with loss: 0.00836930051445961\n",
      "Epoch number: 121 with loss: 0.008176770061254501\n",
      "Epoch number: 131 with loss: 0.007988300174474716\n",
      "Epoch number: 141 with loss: 0.007795250508934259\n",
      "Epoch number: 151 with loss: 0.007569951470941305\n",
      "Epoch number: 161 with loss: 0.0073844357393682\n",
      "Epoch number: 171 with loss: 0.00722264451906085\n",
      "Epoch number: 181 with loss: 0.007072461768984795\n",
      "Epoch number: 191 with loss: 0.0069351461715996265\n",
      "Epoch number: 201 with loss: 0.0068098511546850204\n",
      "Epoch number: 211 with loss: 0.0066964346915483475\n",
      "Epoch number: 221 with loss: 0.006594788283109665\n",
      "Epoch number: 231 with loss: 0.006503491662442684\n",
      "Epoch number: 241 with loss: 0.006423465441912413\n",
      "Epoch number: 251 with loss: 0.0063535599038004875\n",
      "Epoch number: 261 with loss: 0.006292190868407488\n",
      "Epoch number: 271 with loss: 0.006239199545234442\n",
      "Epoch number: 281 with loss: 0.006193933542817831\n",
      "Epoch number: 291 with loss: 0.00615567434579134\n",
      "Epoch number: 301 with loss: 0.006122855469584465\n",
      "Epoch number: 311 with loss: 0.0060947611927986145\n",
      "Epoch number: 321 with loss: 0.006070490460842848\n",
      "Epoch number: 331 with loss: 0.00605009077116847\n",
      "Epoch number: 341 with loss: 0.006032314617186785\n",
      "Epoch number: 351 with loss: 0.0060164486058056355\n",
      "Epoch number: 361 with loss: 0.006001401226967573\n",
      "Epoch number: 371 with loss: 0.005988342687487602\n",
      "Epoch number: 381 with loss: 0.005976556334644556\n",
      "Epoch number: 391 with loss: 0.00596605334430933\n",
      "Epoch number: 401 with loss: 0.005956444889307022\n",
      "Epoch number: 411 with loss: 0.005947459489107132\n",
      "Epoch number: 421 with loss: 0.005939052905887365\n",
      "Epoch number: 431 with loss: 0.0059311268851161\n",
      "Epoch number: 441 with loss: 0.005923356860876083\n",
      "Epoch number: 451 with loss: 0.005916039925068617\n",
      "Epoch number: 461 with loss: 0.005908949300646782\n",
      "Epoch number: 471 with loss: 0.0059020970948040485\n",
      "Epoch number: 481 with loss: 0.0058952877297997475\n",
      "Epoch number: 491 with loss: 0.005888513755053282\n"
     ]
    }
   ],
   "source": [
    "# try a different optimizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7265d5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.112100463177515\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = model(data)\n",
    "        y_pred.append(prediction.argmax().item())\n",
    "        \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088a67c",
   "metadata": {},
   "source": [
    "## 6.\tCompare the performance of the neural networks to the other model you created. Which performed better? Why do you think that is?\n",
    "\n",
    "The model that seemed to perform the best was the Keras model which got down to a MSE of 3.95, which still isn't great but is better than what I was getting with my best performing decision tree regression.\n",
    "\n",
    "I think the Keras model performed the best in part because it is designed to be easier to use for beginner developers and for people with less of a math background. It has been easier for me to understand what I'm doing in terms of manipulating Keras than PyTorch, although despite the poor performance the Pytorch model was performing even worse initially.\n",
    "\n",
    "I think that Keras also performed better than KNN because of the ability to correct using backpropagation.\n",
    "\n",
    "#### UPDATE after running data with min max scaler\n",
    "I didn't go back and try the min max scaler on the keras data but I anticipate that it would improve quite a bit. I still don't feel super confident with PyTorch and I still wonder if it didn't perform as well initially because it's a little more mathy and so didn't handle my unscaled data as well as Keras did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c3d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
